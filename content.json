{"pages":[{"title":"404","text":"","path":"404/index.html","date":"05-07","excerpt":""},{"title":"about","text":"关于我研一在读，转专业至计算机，菜鸟一只","path":"about/index.html","date":"07-17","excerpt":""},{"title":"categories","text":"","path":"categories/index.html","date":"07-17","excerpt":""},{"title":"search","text":"","path":"search/index.html","date":"05-07","excerpt":""},{"title":"tags","text":"","path":"tags/index.html","date":"07-17","excerpt":""}],"posts":[{"title":"化学品预测毒理学平台后台搭建笔记","text":"2019-06-06解决了problem_060501找到后台数据库管理界面（新发现）IP:port/admin problem_060601_unsolved描述： 用户无法注册登录的问题 出错日志： Exception Type: SMTPAuthenticationError Exception Value: (535, &apos;Error: authentication failed&apos;) 原因分析： 邮件服务没有通过认证，无法正常激活。 problem_060602_unsolved描述： openbabel问题不支持png图片生成 出错日志： [2019-06-06 00:10:20,183: ERROR/Worker-15] failed to submit task to prediction model Traceback (most recent call last): File &quot;/home/jiangdong/workspace/CTWS/ChemToolsWebService/chemistry/tasks.py&quot;, line 101, in calculateTask generate_mol_image(task) File &quot;/home/jiangdong/workspace/CTWS/ChemToolsWebService/chemistry/util.py&quot;, line 107, in generate_mol_image mol.draw(show=False, filename=ppath) File &quot;/usr/lib/python2.7/dist-packages/pybel.py&quot;, line 502, in draw raise ImportError(errormessage) ImportError: PNG output format not found. You should compile Open Babel with PNG support. See installation instructions for more information. problem_060603_unsolved问题描述： UnboundLocalError: local variable &apos;suite&apos; referenced before assignment(未解决) 出错日志： [2019-06-06 00:10:20,199: ERROR/MainProcess] Task chemistry.tasks.calculateTask[4898a17f-30b6-4ef7-a3f4-054b12dd719a] raised unexpected: UnboundLocalError(&quot;local variable &apos;suite&apos; referenced before assignment&quot;,) Traceback (most recent call last): File &quot;/home/jiangdong/.local/lib/python2.7/site-packages/celery/app/trace.py&quot;, line 240, in trace_task R = retval = fun(*args, **kwargs) File &quot;/home/jiangdong/.local/lib/python2.7/site-packages/celery/app/trace.py&quot;, line 437, in __protected_call__ return self.run(*args, **kwargs) File &quot;/home/jiangdong/workspace/CTWS/ChemToolsWebService/chemistry/tasks.py&quot;, line 138, in calculateTask suite.status_id = StatusCategory.objects.get(category=STATUS_FAILED) UnboundLocalError: local variable &apos;suite&apos; referenced before assignment 2019-06-05problem_060501_solveddjango报错提示： OperationalError: (1054, &quot;Unknown column &apos;local_search_id&apos; in &apos;field list&apos;&quot;) 出错日志： [2019-06-05 21:38:21,513: ERROR/Worker-15] failed to generate suite_task:1a8292d1-5820-4f01-8743-e131f889ecbd Traceback (most recent call last): File &quot;/home/jiangdong/workspace/CTWS/ChemToolsWebService/chemistry/util.py&quot;, line 451, in generate_calculate_task handle_smile_task(smile, model, sid, local_search_id) File &quot;/home/jiangdong/workspace/CTWS/ChemToolsWebService/chemistry/util.py&quot;, line 352, in handle_smile_task save_record(fpath, model, sid, ORIGIN_SMILE, smile, local_search_id) File &quot;/home/jiangdong/workspace/CTWS/ChemToolsWebService/chemistry/util.py&quot;, line 261, in save_record processed_f.save() 原因分析： 发现任务提交出错发生在chemistry/util.py的save_record函数中 底层错误为： OperationalError: (1054, &quot;Unknown column &apos;local_search_id&apos; in &apos;field list&apos;&quot;) 目前怀疑为数据库中缺少对应的项，正在根据代码核实涉及哪部分的数据库。 解决方法(2019-06-06更新) 进入数据库，运行 alter table chemistry_processedfile add column local_search_id INT(11) NULL; (2019-06-06更新结束) 2019-06-03画图框架需要手动刷新显示，故在html的渲染文件中添加一行温馨提示，添加位置在templates/newtask.html中{\\% include&amp;nbsp “widgets/chem-draw.html” \\%}之后。添加内容为 &lt;font style=”vertical-align: inherit;”&gt;若显示不正常右键绘图区域重新加载框架&lt;/font&gt; 2019-06-02任务列表界面无法访问，发现原因是生成的数据库中的对应表缺少一项，手动在数据库中添加即可。命令如下： mysql –uroot –p root use Chemistry; alter table chemistry_suitetask add column is_hide TINYINT(1) NOT NULL DEFAULT 0; alter table chemistry_singletask add column is_hide TINYINT(1) NOT NULL DEFAULT 0; 2019-06-01一些新的发现？ tools目录下的generate_models.py会根据data文件夹下的csv文件生成chemistry/calcore/matrix/下的py文件。 tools目录下的gen_check.py会根据data文件夹下的csv文件生成tools/data/check下的py文件。 tools/deploy目录下的import_local.py文件好像是导入啥东西的 没有细究。。。 tools/deploy目录下的build_production是用来配置生产环境相关目录的 主要是新建一些相关目录 以及安装对应的依赖包 和初始化数据库。 要安装supervisor sudo pip2 install supervisor python manage.py collectstatic 2019-05-28redis安装及配置redis安装配置django redis配置 apt-get install redis-server 2019-05-18使用不同的settings新建settings文件夹，将原先的settings.py配置成base.py,root_dir向上加一层目录copy dev.py至settings文件夹如下形式启动 python2 manage.py runserver –settings=./settings.settings_dev.py 2019-05-15CPTP平台进展记录 图一 图二 进展说明经过各种依赖库的安装及相关框架的配置，现在已能够在本地访问平台首页，相关测试结果如下： 首页、平台远景、软件帮助这三个页面的内容目前都是一样的…见图一。 图二是“预测计算-新建计算”的页面，但目前还不能提交计算原因可能是Dragon的license还没有配置上，等license配置结束应该就可以正常计算 “预测计算-任务列表”页面访问会出错，估计原因有两种：一是没有预测任务，所以访问出错。二是网站本身链接不存在或者缺少三方包导致访问出错。 分子绘图界面目前显示有些问题，可能是先关文件在源仓库的缺失导致的，需要进一步分析解决。 文件上传功能需要后续继续测试。 接下来的工作 找一台在机房的节点重新部署平台。 等待官网发放license，继续测试计算功能。 阅读相关功能模块源码，找出进展3,4中问题的具体原因。 根据提供的资料，完善平台的首页，平台远景以及软件帮助的静态界面。 2019-05-10今天的思路是采用虚拟环境virtualenv在16.04上使用python2.7配置环境。 添加32位库的支持 改为 sudo apt-get install g++-multilib 2019-04-23下载了bootstrap 目前还不知道有啥用 安装Django官网链接 更改了fail_n*n.png的文件名字，使得仓库能够被完整下载 更新了requirement文件,解决了python三方包的依赖问题但是没有测试三方包能否在现有代码中运行 错误1 描述： -bash: ./tools/build_env.sh: /bin/bash^M: bad interpreter: No such file or directory 解决办法 链接 错误2 描述： 找不到virtualenv: command not found 解决方法 安装virtualenv pip3 install –user virtualenv 总结：现在这份仓库的代码使用的Django-1.6.3和python2的版本，可以说非常的古老，如果要方便后续开发，最好是能转成Django-2.2和python3.5不过这个工作量很多，因为Django2.x相比1.x版本是全新的一个版本，而且只能支持python3，需要慎重考虑。","path":"2019/06/06/化学品预测毒理学平台后台搭建笔记/","date":"06-06","excerpt":"","tags":[{"name":"环境搭建","slug":"环境搭建","permalink":"https://haxisnake.github.io/tags/环境搭建/"},{"name":"Django","slug":"Django","permalink":"https://haxisnake.github.io/tags/Django/"}]},{"title":"设置linux开机启动脚本自动联网认证","text":"由于学校的网络需要认证上网,每次服务器上网都用个人账号认证，因此需要配置下开机启动脚本使其自动上网。(其实就是没钱惹的祸) 一、上网认证的脚本先安装依赖:sudo apt-get install python-pipsudo pip3 install schedulesudo pip3 install argparse task.py： import schedule import time from urllib.parse import quote, unquote import requests import argparse def job(User,Pass): base_url = &apos;http://auth.dlut.edu.cn&apos; try: resp = requests.get(base_url) print(&quot;conneting...&quot;) except Exception: #print(&quot;Already connected...&quot;) return query = resp.text[resp.text.find(&apos;wlanuserip&apos;):resp.text.find(&apos;&lt;/script&gt;&apos;)] query_str = quote(quote(query)) url = base_url + &apos;/eportal/InterFace.do?method=login&apos; data = { &apos;userId&apos;: User, # username 1 &apos;password&apos;: Pass, # password &apos;service&apos;: &apos;&apos;, # empty &apos;queryString&apos;: query_str, &apos;operatorPwd&apos;: &apos;&apos;, # empty &apos;operatorUserId&apos;: &apos;&apos;, # empty &apos;validcode&apos;: &apos;&apos;, # empty } headers = {&apos;Content-Type&apos;: &apos;application/x-www-form-urlencoded; charset=UTF-8&apos;} resp = requests.post(url, data, headers=headers) print(resp.status_code) print(resp.text) parser = argparse.ArgumentParser(description=&apos;connect to dlut&apos;) parser.add_argument(&apos;-u&apos;,type=str, help=&apos;user name&apos;) parser.add_argument(&apos;-p&apos;,type=str, help=&apos;user name&apos;) args = parser.parse_args() User = args.u Pass = args.p job(User, Pass) schedule.every(300).minutes.do(job,User,Pass) while True: schedule.run_pending() time.sleep(300) 其主要作用是进行认证链接,命令行调用格式为: python3 task.py -u username -p passwd username是认证用户名，passwd是对应密码 二、编写bash脚本调用登陆脚本connect.sh: #!/bin/bash python3 /home/sie/netset/task.py -u username -p passwd &amp; 添加可执行权限: chmod +x connect.sh 将connect.sh和task.py都放到同一目录下 设为path_to_sp 三、在/etc/rc.local里添加开机启动脚本命令 配置rc-local.service服务使其能够执行开机启动脚本 参考教程 root权限创建/etc/rc.local文件并写入开机启动的脚本 #!/bin/sh -e # # rc.local # # This script is executed at the end of each multiuser runlevel. # Make sure that the script will &quot;exit 0&quot; on success or any other # value on error. # # In order to enable or disable this script just change the execution # bits. # # By default this script does nothing. su -s /bin/sh sie -c&quot;path_to_sp/connect.sh&quot; exit 0 其中path_to_sp为connect.sh和task.py的目录 至此开机启动配置结束。 以前一直想搞定开机启动脚本的事情，但是总不得其要点，特此记录，以后还要多多了解原理才是。","path":"2019/05/29/设置linux开机启动脚本自动联网认证/","date":"05-29","excerpt":"","tags":[{"name":"服务器维护","slug":"服务器维护","permalink":"https://haxisnake.github.io/tags/服务器维护/"}]},{"title":"服务器环境搭建","text":"ubuntu18.04-servercuda10.1cudann7.5.0 安装显卡驱动 linux查看显卡型号 lshw -numeric -C display 下载nvidia官方驱动 官网链接 bios禁用禁用secure boot 禁用nouveau 打开文件 sudo vim /etc/modprobe.d/blacklist.conf 在最后一行添加: blacklist nouveau 执行如下命令生效 sudo update-initramfs -u 重启 使用命令查看nouveau有没有运行 lsmod | grep nouveau # 没输出代表禁用生效 停止可视化界面 sudo telinit 3 安装驱动 先添加可执行权限 sudo chmod a+x filename.run 执行安装： sudo ./filename.run -no-opengl-files -noopengl-files这个参数一定要加，否则会循环登录 安装CUDA10.1 下载cuda10.1安装文件 官网链接 运行安装 执行安装文件进行安装，同nvidia驱动的执行方式。 注意此时不需要选择安装驱动。驱动版本要注意跟cuda版本相对应。 安装cudann请见官网安装教程","path":"2019/05/07/服务器环境搭建ubuntu18-04-server-cuda10-1-cudann/","date":"05-07","excerpt":"","tags":[]},{"title":"OpenPose环境搭建笔记","text":"OpenPose环境搭建笔记编译caffe recipe for target ‘.build_release/src/caffe/proto/caffe.pb.o’ failed 参考教程：https://blog.csdn.net/w5688414/article/details/79478695 ./include/caffe/util/hdf5.hpp:6:18: fatal error: hdf5.h: No such file or directory 解决办法：在Makefile.config找到以下行并添加蓝色部分 INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-Linux-gnu/hdf5/serial 3.1 如果仍然提示找不到lhdf5和lhdf5_hl(这是因为ubuntu16.04的文件包含位置发生了变化，尤其是需要用到的hdf5的位置，所以需要更改这一路径)选自caffe —找不到lhdf5_hl和lhdf5的错误 解决办法：然后根据情况执行下面两句： cd /usr/lib/x86_64-linux-gnu sudo ln -s libhdf5_serial.so.10.1.0 libhdf5.so sudo ln -s libhdf5_serial_hl.so.10.0.2 libhdf5_hl.so https://github.com/BVLC/caffe/issues/6359 /usr/include/c++/5/typeinfo:39:37: error: expected ‘}’ before end of line/usr/include/c++/5/typeinfo:39:37: error: expected unqualified-id before end of line/usr/include/c++/5/typeinfo:39:37: error: expected ‘}’ before end of line/usr/include/c++/5/typeinfo:39:37: error: expected ‘}’ before end of line/usr/include/c++/5/typeinfo:39:37: error: expected ‘}’ before end of line/usr/include/c++/5/typeinfo:39:37: error: expected declaration before end of lineMakefile:588: recipe for target ‘.build_release/src/caffe/proto/caffe.pb.o’ failedmake: *** [.build_release/src/caffe/proto/caffe.pb.o] Error 1 CXXFLAGS += -pthread -fPIC $(COMMON_FLAGS) $(WARNINGS) -std=c++11NVCCFLAGS += -D_FORCE_INLINES -ccbin=$(CXX) -Xcompiler -fPIC $(COMMON_FLAGS) -std=c++11LINKFLAGS += -pthread -fPIC $(COMMON_FLAGS) $(WARNINGS) -std=c++11 Unsupported gpu architecture ‘compute_20’ 注释Makefile.config 中 : # CUDA architecture setting: going with all of them. # For CUDA &lt; 6.0, comment the *_50 through *_61 lines for compatibility. # For CUDA &lt; 8.0, comment the *_60 and *_61 lines for compatibility. # For CUDA &gt;= 9.0, comment the *_20 and *_21 lines for compatibility. CUDA_ARCH := -gencode arch=compute_30,code=sm_30 \\ -gencode arch=compute_35,code=sm_35 \\ -gencode arch=compute_50,code=sm_50 \\ -gencode arch=compute_52,code=sm_52 \\ -gencode arch=compute_60,code=sm_60 \\ -gencode arch=compute_61,code=sm_61 \\ -gencode arch=compute_61,code=compute_61 leveldbhttps://blog.csdn.net/oeljeklaus/article/details/78802922 https://blog.csdn.net/fogxcg/article/details/75332146 build opencv3.4 cmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/local -D PYTHON3_EXECUTABLE=/usr/bin/python3 -D PYTHON_INCLUDE_DIR=/usr/include/python3 -D PYTHON_INCLUDE_DIR2=/usr/include/x86_64-linux-gnu/python3.5m -D PYTHON_LIBRARY=/usr/lib/x86_64-linux-gnu/libpython3.5m.so -D PYTHON3_NUMPY_INCLUDE_DIRS=/usr/lib/python3/dist-packages/numpy/core/include/ levelDB出错 https://blog.csdn.net/jyl1999xxxx/article/details/80583465 glog 和 gflags 的安装https://www.cnblogs.com/burningTheStar/p/6986048.html-fPIChttps://github.com/BVLC/caffe/issues/2171 make runtest .build_release/tools/caffe: error while loading shared libraries: libopencv_imgcodecs.so.3.4: cannot open shared object file: No such file or directoryMakefile:542: recipe for target ‘runtest’ failed make runtest https://blog.csdn.net/wonengguwozai/article/details/52724409 编译openpose cuda结构不共存 Unsupported gpu architecture ‘compute_xx’ 在cmake-gui 中选择 CUDA_ARCH 为mannul 然后再删掉其中不符合的结构 g++: error trying to exec ‘cc1plus’: execvp: 没有那个文件或目录 gcc g++ 版本不兼容 https://blog.csdn.net/yile0000/article/details/80105625 nvidia驱动 https://blog.csdn.net/wf19930209/article/details/81877822 sudo service lightdm stop gflags.cc is being linked both statically and dynamically https://github.com/google/glog/issues/53 修改CmakeCache.txt中的GFLAGS_LIBRARY:FILEPATH=/usr/local/lib/libgflags.a改为：GFLAGS_LIBRARY:FILEPATH=/usr/local/lib/libgflags.so然后重新编译 [libprotobuf ERROR google/protobuf/message_lite.cc:123] Can’t parse message of type “caffe.NetParameter” because it is missing required fields: layer[0].clip_param.min, layer[0].clip_param.max 要用openpose自带的caffe库 选择openpose编译caffe时选项可以在3rdparty/caffe中设置编译参数","path":"2019/04/09/OpenPose环境搭建笔记/","date":"04-09","excerpt":"","tags":[]},{"title":"智能教室项目记录","text":"问题：放入新的数据时，训练的loss输出值总是nan 尝试： 调小学习率 未解决 2019-3-25 现在发现数据的标签中有nan值，怀疑是数据集问题，打算对自己的数据集进行检查 经过检查后发现，在从cvat的标签文件转为voc格式的标签文件时，边框的长宽颠倒导致数据出错。 2019年4月1日经过几天的训练，SSD网络在我们自己构造的数据上已经有了很好的表现，能在测试集上达到90.09的MAP，但是发现相比于原先在VOC上的网络模型，新训练出来的模型对于小目标的识别有了较大改善，但是对于大目标的识别率有所下降，因此今天写了从VOC中提取特定种类图片的脚本，打算将其他数据集中跟人体有关的数据纳入到新的数据集中，削减教室中图片数量，提升数据集的多样性。","path":"2019/03/26/智能教室项目记录/","date":"03-26","excerpt":"","tags":[]},{"title":"docker无法启动问题调试","text":"Job for docker.service failed. See &quot;systemctl status docker.service&quot; and &quot;journalctl -xe&quot; for details. Error starting daemon: error while opening volume store metadata database: timeout ps axf | grep docker | grep -v grep | awk &apos;{print &quot;kill -9 &quot; $1}&apos; | sudo sh sudo systemctl start docker","path":"2019/02/18/docker无法启动问题调试/","date":"02-18","excerpt":"","tags":[]},{"title":"C++ vector size()与有符号数比较要注意的问题","text":"C++ vector size()与有符号数比较要注意的问题今天在刷题时要写一个循环体，为了保证能包括初始情况，其中一个循环变量要从j=-1开始计数，但是导致终止条件j&lt;num.size(),一直为false,后经过调试发现是因为vector的size()方法返回的是无符号整形，因此与有符号数进行比较时要显式的进行一次类型转换。即j&lt;(int)num.size(),即要注意下面代码中第一个循环体结束语句的写法。 12345678910111213141516171819202122232425class Solution &#123;public: int minSubArrayLen(int s, vector&lt;int&gt;&amp; nums) &#123; if(0==nums.size()) return 0; int sum=0; int subarraylen=nums.size()+1; for(int i=0,j=-1;(i&lt;nums.size()) &amp;&amp; (j&lt;(int)nums.size());)&#123; if(sum&lt;s)&#123; sum+=nums[++j]; &#125;else&#123; if(i==j)&#123; return 1; &#125; subarraylen=(j-i+1)&lt;subarraylen?(j-i+1):subarraylen; sum-=nums[i++]; &#125; &#125; if(subarraylen!=nums.size()+1)&#123; return subarraylen; &#125;else&#123; return 0; &#125; &#125;&#125;;","path":"2019/01/14/C-vector-size-与有符号数比较要注意的问题/","date":"01-14","excerpt":"","tags":[{"name":"bug","slug":"bug","permalink":"https://haxisnake.github.io/tags/bug/"},{"name":"c++","slug":"c","permalink":"https://haxisnake.github.io/tags/c/"}]},{"title":"使用python搭建小型搜索引擎","text":"项目代码链接 一、目标为准备信息检索课程的期末大作业，因此我使用python搭建了一个小型的搜索引擎，其功能是检索大工新闻网的新闻。 二、原理和工具简介2.1 原理该搜索引擎的原理是采用scrapy对大工新闻网进行爬虫，提取出文字新闻，并将新闻内容存入数据库A，再利用Django框架搭建一个搜索服务器，在服务器上部署Haystack+Whoosh搜索引擎，使用jieba分词工具来进行中文分词和停用词过滤。通过搜索引擎工具建立索引文件后，在前端完成用户交互界面，实现一个完整的小型搜索引擎。 2.2 工具简介 Scrapy Python开发的一个快速、高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。 在本项目中用来爬取网站数据。 Django Django是一个开放源代码的Web应用框架，由Python写成。采用了MVC的框架模式，即模型M，视图V和控制器C。 在本项目用来做搜索服务器的框架。 Haystack+Whoosh Haystack是一个Django上的应用，可以用于整合搜索引擎，它只依赖于它自身的代码，利用它可以切换不同的搜索引擎工具。Whoosh是一个索引文本和搜索文本的类库，它可以提供搜索文本的服务。 在本项目中使用Haystack将Whoosh部署到Django服务器作为搜索引擎后端。 Jieba Jieba是一个python实现的分词库，对中文有着很强大的分词能力。 在本项目用于对新闻进行中文分词和停用词过滤。 三、开发环境和运行环境3.1 开发环境 win 10 python 3.6.5 scrapy 1.5.1 安装教程 Django 2.1.3 安装教程 Haystack+Whoosh 配置教程 Jieba 0.39 配置教程 3.2 运行环境同开发环境 四、系统模块4.1 网络爬虫模块定义用于Scrapy爬虫的数据类型，包括链接、标题和文章内容:1234class NewsItem(scrapy.Item): url = scrapy.Field() title = scrapy.Field() content = scrapy.Field() 编写爬虫策略:爬虫策略的制定依据于网页源代码的链接形式，由于要爬取的是文字类新闻，所以要跟进与文字类新闻的链接。对于具体的新闻页利用回调函数爬取其链接、标题和内容。而对于新闻列表页，需要跟进下一页的链接。具体规则代码如下: 12345678910111213141516171819202122class NewsSpider(CrawlSpider): print(\"news spider starting\") name = 'news' allowed_domains = ['news.dlut.edu.cn'] start_urls = ['http://news.dlut.edu.cn/'] rules = ( # 对于新闻页链接进行跟进 Rule(LinkExtractor(allow=(\"xw/[a-z]+.htm\"))), # 对于详细新闻页利用parse_item回调函数进行内容爬取 Rule(LinkExtractor(allow=(\"info/\\d&#123;4,&#125;/\\d&#123;3,&#125;\\.htm\")),callback=\"parse_item\"), # 对于新闻列表中的下一页链接进行跟进 Rule(LinkExtractor(allow=(\"\\d&#123;1,&#125;.htm\"),restrict_xpaths=\"//a[@class='Next']\")), ) def parse_item(self, response): self.log(\"Hi, this is a new page! %s\"% response.url) item = NewsItem() item['title'] = response.xpath('/html/head/title/text()').extract()[0] item['url'] = response.url item['content']=response.xpath(\"//div[@class='cont-detail fs-small']/p/text()\").extract() yield item 使用pipline机制将数据保存至数据库当中1234567891011121314151617181920212223242526272829class SpiderprojectPipeline(object): def process_item(self, item, spider): if spider.name == 'news': conn = sqlite3.connect('db.sqlite3') cursor = conn.cursor() title = item['title'] url = item['url'] content_tmp = item['content'] content=\"\" for p in content_tmp: content+=p.strip() sql_search = 'select arturl from search_article where arturl==\"%s\"' % (url) sql = 'insert into articles_article(title, content, arturl) values(\"%s\", \"%s\", \"%s\")'%(title, content, url) try: #如果当前数据库中不存在该条新闻，则将新闻保存至数据库当中 cursor.execute(sql_search) result_search = cursor.fetchone() if result_search is None or result_search[0].strip()=='': cursor.execute(sql) result=cursor.fetchone() conn.commit() cursor.execute(sql) result=cursor.fetchone() conn.commit() except Exception as e: print(\"&gt;&gt;&gt; catch exception !\") print(e) conn.rollback() return item 4.2 搜索模块在要进行搜索的应用的models.py文件中建立model类用来表示要进行搜索的新闻文章 1234class Article(models.Model): title = models.CharField(max_length=50) arturl = models.CharField(max_length=200) content = models.CharField(max_length=1000) 同时使用django命令python manage.py makemigrations和python manage.py migrate生成数据库文件，并用爬虫得到的数据库替换生成的数据库。 在django框架中配置Haystack+Whoosh来引入搜索模块 123456789101112# 配置搜索引擎后端HAYSTACK_CONNECTIONS=&#123; 'default':&#123; 'ENGINE': 'articles.whoosh_cn_backend.WhooshEngine', # 索引文件路径 'PATH': os.path.join(BASE_DIR, 'whoosh_index'), # 在项目目录下创建文件夹 whoosh_index &#125;&#125;# 当添加、修改、删除数据时，自动生成索引HAYSTACK_SIGNAL_PROCESSOR = 'haystack.signals.RealtimeSignalProcessor'# 每页显示十条搜索结果HAYSTACK_SEARCH_RESULTS_PER_PAGE = 10 在要进行搜索的应用的目录下建立search_indexes.py文件 123456789101112from haystack import indexesfrom articles.models import Articleclass ArticleIndex(indexes.SearchIndex, indexes.Indexable): #类名必须为需要检索的Model_name+Index，这里需要检索Article，所以创建ArticleIndex text = indexes.CharField(document=True, use_template=True) #创建一个text字段 def get_model(self): #重载get_model方法，必须要有！ return Article def index_queryset(self, using=None): #重载index_..函数 \"\"\"Used when the entire index for model is updated.\"\"\" return self.get_model().objects.all() 在articles\\templates\\search\\indexes\\articles\\下建立article_text.txt文件确定搜索内容 123&#123;&#123; object.title &#125;&#125;&#123;&#123; object.content &#125;&#125;&#123;&#123; object.url &#125;&#125; 4.2 预处理模块使用jieba来进行中文分词，需要在whoosh_cn_backend文件中替换StemmingAnalyzer为ChineseAnalyzer 同时引入停用词表，配置ChineseAnalyzer使其支持停用词过滤 1234567891011121314151617181920212223242526import jiebaimport reimport os# 导入停用词过滤表stop_file_dir=os.path.dirname(os.path.dirname(os.path.abspath(__file__)))STOP_WORDS = frozenset([line.strip() for line in open(os.path.join(stop_file_dir, 'stop.txt'),'r',encoding='gbk').readlines()])accepted_chars = re.compile(r\"[\\u4E00-\\u9FD5]+\")class ChineseTokenizer(Tokenizer): def __call__(self, text, **kargs): words = jieba.tokenize(text, mode=\"search\") token = Token() for (w, start_pos, stop_pos) in words: if not accepted_chars.match(w) and len(w) &lt;= 1: continue token.original = token.text = w token.pos = start_pos token.startchar = start_pos token.endchar = stop_pos yield token def ChineseAnalyzer(stoplist=STOP_WORDS, minsize=1, stemfn=stem, cachesize=50000): return (ChineseTokenizer() | LowercaseFilter() | StopFilter(stoplist=stoplist, minsize=minsize) | StemFilter(stemfn=stemfn, ignore=None, cachesize=cachesize)) 配置完成后使用python manage.py rebuild_index建立搜索索引 4.4 交互模块搜索首页html关键代码： 12345&lt;form method='get' action=\"/search/\" target=\"_blank\"&gt; &lt;input type=\"text\" name=\"q\"&gt; &lt;br&gt; &lt;input type=\"submit\" value=\"查询\"&gt;&lt;/form&gt; 首页如下图所示： 查询结果页html关键代码： 12345678910111213141516171819&#123;% load highlight %&#125;&lt;h3&gt;搜索&amp;nbsp;&lt;b&gt;&#123;&#123;query&#125;&#125;&lt;/b&gt;&amp;nbsp;结果如下：&lt;/h3&gt;&lt;ul&gt;&#123;%for item in page%&#125; &lt;li&gt;&#123;&#123;item.object.title|safe&#125;&#125;&lt;/li&gt; &lt;p&gt;&#123;% highlight item.object.content with query %&#125;&lt;/p&gt; &lt;a href=&quot;&#123;&#123;item.object.arturl&#125;&#125;&quot;&gt;&#123;&#123;item.object.arturl&#125;&#125;&lt;/a&gt;&#123;%empty%&#125; &lt;li&gt;啥也没找到&lt;/li&gt;&#123;%endfor%&#125;&lt;/ul&gt;&lt;hr&gt;&#123;%for pindex in page.paginator.page_range%&#125; &#123;%if pindex == page.number%&#125; &#123;&#123;pindex&#125;&#125;&amp;nbsp;&amp;nbsp; &#123;%else%&#125; &lt;a href=&quot;?q=&#123;&#123;query&#125;&#125;&amp;amp;page=&#123;&#123;pindex&#125;&#125;&quot;&gt;&#123;&#123;pindex&#125;&#125;&lt;/a&gt;&amp;nbsp;&amp;nbsp; &#123;%endif%&#125;&#123;%endfor%&#125; 其中使用 12&#123;% load highlight %&#125;&lt;p&gt;&#123;% highlight item.object.content with query %&#125;&lt;/p&gt; 进行搜索结果的高亮显示 最终搜索页面如下图所示： 五、项目代码代码链接","path":"2018/12/31/使用python搭建小型搜索引擎/","date":"12-31","excerpt":"","tags":[{"name":"Python","slug":"Python","permalink":"https://haxisnake.github.io/tags/Python/"},{"name":"Scrapy","slug":"Scrapy","permalink":"https://haxisnake.github.io/tags/Scrapy/"},{"name":"Django","slug":"Django","permalink":"https://haxisnake.github.io/tags/Django/"},{"name":"Haystack","slug":"Haystack","permalink":"https://haxisnake.github.io/tags/Haystack/"},{"name":"Whoosh","slug":"Whoosh","permalink":"https://haxisnake.github.io/tags/Whoosh/"},{"name":"Jieba","slug":"Jieba","permalink":"https://haxisnake.github.io/tags/Jieba/"}]},{"title":"树莓派3B——turtlebot3——ROS开发环境搭建问题记录","text":"由于一个比赛项目需要在ROS上做开发，但是按照网上教程搭建开发环境出现了一些问题，因此在此做简单整理，本笔记的开发环境如下： 硬件环境：turtlebot3 burger(用的其自带的树莓派为raspberry3B) 树莓派系统版本：ubuntu-mate-16.04.2-desktop-armhf PC系统版本： ubuntu-16.04.3-desktop-amd64 教程参考： https://www.ncnynl.com/archives/201702/1392.html 问题记录问题一问题描述：wget 安装脚本时验证出错导致脚本无法下载 解决方法：在wget命令后加上–no-check-certificate选项即可 问题二问题描述：git clone 出现CAfile: /etc/ssl/certs/ca-certificates.crt CRLfile: none错误 解决方式：运行 export GIT_SSL_NO_VERIFY=1 问题三问题描述：安装turtlebot依赖包之后找不到catkin_make命令 解决方式：重新执行 source /opt/ros/kinetic/setup.sh 问题四问题描述：cakin_make时找不到interactive_maker模块 解决方式：修改安装脚本，使其安装desktop_full版本 问题五问题描述：checksum error when launching turtlebot3_bringup 解决方式：更新OpenCR固件至最新版与ROS版本相匹配","path":"2018/07/18/树莓派3B-turtlebot3ros开发环境搭建问题记录/","date":"07-18","excerpt":"","tags":[{"name":"环境搭建","slug":"环境搭建","permalink":"https://haxisnake.github.io/tags/环境搭建/"},{"name":"raspberry3B","slug":"raspberry3B","permalink":"https://haxisnake.github.io/tags/raspberry3B/"},{"name":"ROS","slug":"ROS","permalink":"https://haxisnake.github.io/tags/ROS/"},{"name":"turtlebot3","slug":"turtlebot3","permalink":"https://haxisnake.github.io/tags/turtlebot3/"}]},{"title":"opencv3-ubuntu16.04-install","text":"123456789101112131415161718sudo apt-get install cmakesudo apt-get install python3-dev python3-numpysudo apt-get install gcc g++sudo apt-get install libgtk2.0-devsudo apt-get install libv4l-devsudo apt-get install libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-devsudo apt-get install sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev liblapacke-devsudo apt-get install libxvidcore-dev libx264-dev sudo apt-get install libatlas-base-dev gfortransudo apt-get install ffmpeg sudo apt-get install gitgit clone https://github.com/opencv/opencv.gitcmake dir/of/opencv/sourcesudo make -j4 sudo make install","path":"2018/07/17/opencv3-ubuntu16-04-install/","date":"07-17","excerpt":"","tags":[{"name":"环境搭建","slug":"环境搭建","permalink":"https://haxisnake.github.io/tags/环境搭建/"},{"name":"opencv3","slug":"opencv3","permalink":"https://haxisnake.github.io/tags/opencv3/"}]},{"title":"树莓派3B+编译安装opencv3","text":"一、更新源12mv sources.list /etc/apt/sources.list mv raspi.list /etc/apt/sources.list.d/raspi.list 更新源的配置，注意文件存放的位置文件sources.list和raspi.list具体内容如下 sources.list文件: deb http://mirrors.ustc.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi deb http://mirrors.aliyun.com/raspbian/raspbian/ stretch main contrib non-free rpi deb http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi deb http://mirrors.neusoft.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi raspi.list文件:1234deb http://mirrors.ustc.edu.cn/raspbian/raspbian/ stretch main uideb http://mirrors.aliyun.com/raspbian/raspbian/ stretch main uideb http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ stretch main uideb http://mirrors.neusoft.edu.cn/raspbian/raspbian/ stretch main ui 在终端执行更新命令:12sudo apt-get updatesudo apt-get upgrade 二、安装依赖包123456789sudo apt-get install build-essential cmake git pkg-config sudo apt-get install libjpeg8-dev sudo apt-get install libtiff5-dev sudo apt-get install libjasper-dev sudo apt-get install libpng12-devsudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-devsudo apt-get install libgtk2.0-devsudo apt-get install libatlas-base-dev gfortran 注意：降级安装有些安装包依赖的版本低需要降级安装，如下，对depends后面的进行降级安装1sudo aptitude install xxxx 三、下载源码1git clone https://github.com/opencv/opencv.git 四、编译1234cmake dir/of/opencv/sourcesudo make -j4 sudo make installsudo ldconfig","path":"2018/07/17/树莓派3B-编译安装opencv3/","date":"07-17","excerpt":"","tags":[{"name":"环境搭建","slug":"环境搭建","permalink":"https://haxisnake.github.io/tags/环境搭建/"},{"name":"opencv3","slug":"opencv3","permalink":"https://haxisnake.github.io/tags/opencv3/"},{"name":"raspberry3B+","slug":"raspberry3B","permalink":"https://haxisnake.github.io/tags/raspberry3B/"}]}]}