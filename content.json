{"meta":{"title":"抗锅 填坑 修路","subtitle":null,"description":null,"author":"姜东/Jiang Dong","url":"https://haxisnake.github.io"},"pages":[{"title":"about","date":"2018-07-17T09:50:51.000Z","updated":"2018-07-17T09:54:06.747Z","comments":true,"path":"about/index.html","permalink":"https://haxisnake.github.io/about/index.html","excerpt":"","text":"关于我大四毕业生，转专业至计算机，菜鸟一只"},{"title":"tags","date":"2018-07-17T10:08:17.000Z","updated":"2018-07-17T11:01:42.173Z","comments":false,"path":"tags/index.html","permalink":"https://haxisnake.github.io/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2018-07-17T10:21:16.000Z","updated":"2018-07-17T10:48:07.095Z","comments":true,"path":"categories/index.html","permalink":"https://haxisnake.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"使用python搭建小型搜索引擎","slug":"使用python搭建小型搜索引擎","date":"2018-12-31T05:19:06.000Z","updated":"2018-12-31T16:57:52.698Z","comments":true,"path":"2018/12/31/使用python搭建小型搜索引擎/","link":"","permalink":"https://haxisnake.github.io/2018/12/31/使用python搭建小型搜索引擎/","excerpt":"","text":"一、目标为准备信息检索课程的期末大作业，因此我使用python搭建了一个小型的搜索引擎，其功能是检索大工新闻网的新闻。 二、原理和工具简介2.1 原理该搜索引擎的原理是采用scrapy对大工新闻网进行爬虫，提取出文字新闻，并将新闻内容存入数据库A，再利用Django框架搭建一个搜索服务器，在服务器上部署Haystack+Whoosh搜索引擎，使用jieba分词工具来进行中文分词和停用词过滤。通过搜索引擎工具建立索引文件后，在前端完成用户交互界面，实现一个完整的小型搜索引擎。 2.2 工具简介 Scrapy Python开发的一个快速、高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。 在本项目中用来爬取网站数据。 Django Django是一个开放源代码的Web应用框架，由Python写成。采用了MVC的框架模式，即模型M，视图V和控制器C。 在本项目用来做搜索服务器的框架。 Haystack+Whoosh Haystack是一个Django上的应用，可以用于整合搜索引擎，它只依赖于它自身的代码，利用它可以切换不同的搜索引擎工具。Whoosh是一个索引文本和搜索文本的类库，它可以提供搜索文本的服务。 在本项目中使用Haystack将Whoosh部署到Django服务器作为搜索引擎后端。 Jieba Jieba是一个python实现的分词库，对中文有着很强大的分词能力。 在本项目用于对新闻进行中文分词和停用词过滤。 三、开发环境和运行环境3.1 开发环境 win 10 python 3.6.5 scrapy 1.5.1 安装教程 Django 2.1.3 安装教程 Haystack+Whoosh 配置教程 Jieba 0.39 配置教程 3.2 运行环境同开发环境 四、系统模块4.1 网络爬虫模块定义用于Scrapy爬虫的数据类型，包括链接、标题和文章内容:class NewsItem(scrapy.Item): url = scrapy.Field() title = scrapy.Field() content = scrapy.Field() 编写爬虫策略:爬虫策略的制定依据于网页源代码的链接形式，由于要爬取的是文字类新闻，所以要跟进与文字类新闻的链接。对于具体的新闻页利用回调函数爬取其链接、标题和内容。而对于新闻列表页，需要跟进下一页的链接。具体规则代码如下: class NewsSpider(CrawlSpider): print(&quot;news spider starting&quot;) name = &apos;news&apos; allowed_domains = [&apos;news.dlut.edu.cn&apos;] start_urls = [&apos;http://news.dlut.edu.cn/&apos;] rules = ( # 对于新闻页链接进行跟进 Rule(LinkExtractor(allow=(&quot;xw/[a-z]+.htm&quot;))), # 对于详细新闻页利用parse_item回调函数进行内容爬取 Rule(LinkExtractor(allow=(&quot;info/\\d{4,}/\\d{3,}\\.htm&quot;)),callback=&quot;parse_item&quot;), # 对于新闻列表中的下一页链接进行跟进 Rule(LinkExtractor(allow=(&quot;\\d{1,}.htm&quot;),restrict_xpaths=&quot;//a[@class=&apos;Next&apos;]&quot;)), ) def parse_item(self, response): self.log(&quot;Hi, this is a new page! %s&quot;% response.url) item = NewsItem() item[&apos;title&apos;] = response.xpath(&apos;/html/head/title/text()&apos;).extract()[0] item[&apos;url&apos;] = response.url item[&apos;content&apos;]=response.xpath(&quot;//div[@class=&apos;cont-detail fs-small&apos;]/p/text()&quot;).extract() yield item 使用pipline机制将数据保存至数据库当中class SpiderprojectPipeline(object): def process_item(self, item, spider): if spider.name == &apos;news&apos;: conn = sqlite3.connect(&apos;db.sqlite3&apos;) cursor = conn.cursor() title = item[&apos;title&apos;] url = item[&apos;url&apos;] content_tmp = item[&apos;content&apos;] content=&quot;&quot; for p in content_tmp: content+=p.strip() sql_search = &apos;select arturl from search_article where arturl==&quot;%s&quot;&apos; % (url) sql = &apos;insert into articles_article(title, content, arturl) values(&quot;%s&quot;, &quot;%s&quot;, &quot;%s&quot;)&apos;%(title, content, url) try: #如果当前数据库中不存在该条新闻，则将新闻保存至数据库当中 cursor.execute(sql_search) result_search = cursor.fetchone() if result_search is None or result_search[0].strip()==&apos;&apos;: cursor.execute(sql) result=cursor.fetchone() conn.commit() cursor.execute(sql) result=cursor.fetchone() conn.commit() except Exception as e: print(&quot;&gt;&gt;&gt; catch exception !&quot;) print(e) conn.rollback() return item 4.2 搜索模块在要进行搜索的应用的models.py文件中建立model类用来表示要进行搜索的新闻文章 class Article(models.Model): title = models.CharField(max_length=50) arturl = models.CharField(max_length=200) content = models.CharField(max_length=1000) 同时使用django命令python manage.py makemigrations和python manage.py migrate生成数据库文件，并用爬虫得到的数据库替换生成的数据库。 在django框架中配置Haystack+Whoosh来引入搜索模块 # 配置搜索引擎后端 HAYSTACK_CONNECTIONS={ &apos;default&apos;:{ &apos;ENGINE&apos;: &apos;articles.whoosh_cn_backend.WhooshEngine&apos;, # 索引文件路径 &apos;PATH&apos;: os.path.join(BASE_DIR, &apos;whoosh_index&apos;), # 在项目目录下创建文件夹 whoosh_index } } # 当添加、修改、删除数据时，自动生成索引 HAYSTACK_SIGNAL_PROCESSOR = &apos;haystack.signals.RealtimeSignalProcessor&apos; # 每页显示十条搜索结果 HAYSTACK_SEARCH_RESULTS_PER_PAGE = 10 在要进行搜索的应用的目录下建立search_indexes.py文件 from haystack import indexes from articles.models import Article class ArticleIndex(indexes.SearchIndex, indexes.Indexable): #类名必须为需要检索的Model_name+Index，这里需要检索Article，所以创建ArticleIndex text = indexes.CharField(document=True, use_template=True) #创建一个text字段 def get_model(self): #重载get_model方法，必须要有！ return Article def index_queryset(self, using=None): #重载index_..函数 &quot;&quot;&quot;Used when the entire index for model is updated.&quot;&quot;&quot; return self.get_model().objects.all() 在articles\\templates\\search\\indexes\\articles\\下建立article_text.txt文件确定搜索内容 {{ object.title }} {{ object.content }} {{ object.url }} 4.2 预处理模块使用jieba来进行中文分词，需要在whoosh_cn_backend文件中替换StemmingAnalyzer为ChineseAnalyzer 同时引入停用词表，配置ChineseAnalyzer使其支持停用词过滤 import jieba import re import os # 导入停用词过滤表 stop_file_dir=os.path.dirname(os.path.dirname(os.path.abspath(__file__))) STOP_WORDS = frozenset([line.strip() for line in open(os.path.join(stop_file_dir, &apos;stop.txt&apos;),&apos;r&apos;,encoding=&apos;gbk&apos;).readlines()]) accepted_chars = re.compile(r&quot;[\\u4E00-\\u9FD5]+&quot;) class ChineseTokenizer(Tokenizer): def __call__(self, text, **kargs): words = jieba.tokenize(text, mode=&quot;search&quot;) token = Token() for (w, start_pos, stop_pos) in words: if not accepted_chars.match(w) and len(w) &lt;= 1: continue token.original = token.text = w token.pos = start_pos token.startchar = start_pos token.endchar = stop_pos yield token def ChineseAnalyzer(stoplist=STOP_WORDS, minsize=1, stemfn=stem, cachesize=50000): return (ChineseTokenizer() | LowercaseFilter() | StopFilter(stoplist=stoplist, minsize=minsize) | StemFilter(stemfn=stemfn, ignore=None, cachesize=cachesize)) 配置完成后使用python manage.py rebuild_index建立搜索索引 4.4 交互模块搜索首页html关键代码： &lt;form method=&apos;get&apos; action=&quot;/search/&quot; target=&quot;_blank&quot;&gt; &lt;input type=&quot;text&quot; name=&quot;q&quot;&gt; &lt;br&gt; &lt;input type=&quot;submit&quot; value=&quot;查询&quot;&gt; &lt;/form&gt; 首页如下图所示： 查询结果页html关键代码： {% load highlight %} &lt;h3&gt;搜索&amp;nbsp;&lt;b&gt;{{query}}&lt;/b&gt;&amp;nbsp;结果如下：&lt;/h3&gt; &lt;ul&gt; {%for item in page%} {{item.object.title|safe}} {% highlight item.object.content with query %} {{item.object.arturl}} {%empty%} 啥也没找到 {%endfor%} &lt;/ul&gt; &lt;hr&gt; {%for pindex in page.paginator.page_range%} {%if pindex == page.number%} {{pindex}}&nbsp;&nbsp; {%else%} {{pindex}}&nbsp;&nbsp; {%endif%} {%endfor%} 其中使用 {% load highlight %} &lt;p&gt;{% highlight item.object.content with query %}&lt;/p&gt; 进行搜索结果的高亮显示 最终搜索页面如下图所示：","categories":[{"name":"项目总结","slug":"项目总结","permalink":"https://haxisnake.github.io/categories/项目总结/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://haxisnake.github.io/tags/Python/"},{"name":"Scrapy","slug":"Scrapy","permalink":"https://haxisnake.github.io/tags/Scrapy/"},{"name":"Django","slug":"Django","permalink":"https://haxisnake.github.io/tags/Django/"},{"name":"Haystack","slug":"Haystack","permalink":"https://haxisnake.github.io/tags/Haystack/"},{"name":"Whoosh","slug":"Whoosh","permalink":"https://haxisnake.github.io/tags/Whoosh/"},{"name":"Jieba","slug":"Jieba","permalink":"https://haxisnake.github.io/tags/Jieba/"}]},{"title":"树莓派3B——turtlebot3——ROS开发环境搭建问题记录","slug":"树莓派3B-turtlebot3ros开发环境搭建问题记录","date":"2018-07-18T05:41:27.000Z","updated":"2018-07-18T06:02:41.752Z","comments":true,"path":"2018/07/18/树莓派3B-turtlebot3ros开发环境搭建问题记录/","link":"","permalink":"https://haxisnake.github.io/2018/07/18/树莓派3B-turtlebot3ros开发环境搭建问题记录/","excerpt":"","text":"由于一个比赛项目需要在ROS上做开发，但是按照网上教程搭建开发环境出现了一些问题，因此在此做简单整理，本笔记的开发环境如下： 硬件环境：turtlebot3 burger(用的其自带的树莓派为raspberry3B) 树莓派系统版本：ubuntu-mate-16.04.2-desktop-armhf PC系统版本： ubuntu-16.04.3-desktop-amd64 教程参考： https://www.ncnynl.com/archives/201702/1392.html 问题记录问题一问题描述：wget 安装脚本时验证出错导致脚本无法下载 解决方法：在wget命令后加上–no-check-certificate选项即可 问题二问题描述：git clone 出现CAfile: /etc/ssl/certs/ca-certificates.crt CRLfile: none错误 解决方式：运行 export GIT_SSL_NO_VERIFY=1 问题三问题描述：安装turtlebot依赖包之后找不到catkin_make命令 解决方式：重新执行 source /opt/ros/kinetic/setup.sh 问题四问题描述：cakin_make时找不到interactive_maker模块 解决方式：修改安装脚本，使其安装desktop_full版本 问题五问题描述：checksum error when launching turtlebot3_bringup 解决方式：更新OpenCR固件至最新版与ROS版本相匹配","categories":[{"name":"调试记录","slug":"调试记录","permalink":"https://haxisnake.github.io/categories/调试记录/"}],"tags":[{"name":"环境搭建","slug":"环境搭建","permalink":"https://haxisnake.github.io/tags/环境搭建/"},{"name":"raspberry3B","slug":"raspberry3B","permalink":"https://haxisnake.github.io/tags/raspberry3B/"},{"name":"ROS","slug":"ROS","permalink":"https://haxisnake.github.io/tags/ROS/"},{"name":"turtlebot3","slug":"turtlebot3","permalink":"https://haxisnake.github.io/tags/turtlebot3/"}]},{"title":"opencv3-ubuntu16.04-install","slug":"opencv3-ubuntu16-04-install","date":"2018-07-17T11:45:44.000Z","updated":"2018-07-17T11:53:39.914Z","comments":true,"path":"2018/07/17/opencv3-ubuntu16-04-install/","link":"","permalink":"https://haxisnake.github.io/2018/07/17/opencv3-ubuntu16-04-install/","excerpt":"","text":"123456789101112131415161718sudo apt-get install cmakesudo apt-get install python3-dev python3-numpysudo apt-get install gcc g++sudo apt-get install libgtk2.0-devsudo apt-get install libv4l-devsudo apt-get install libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-devsudo apt-get install sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev liblapacke-devsudo apt-get install libxvidcore-dev libx264-dev sudo apt-get install libatlas-base-dev gfortransudo apt-get install ffmpeg sudo apt-get install gitgit clone https://github.com/opencv/opencv.gitcmake dir/of/opencv/sourcesudo make -j4 sudo make install","categories":[{"name":"调试记录","slug":"调试记录","permalink":"https://haxisnake.github.io/categories/调试记录/"}],"tags":[{"name":"环境搭建","slug":"环境搭建","permalink":"https://haxisnake.github.io/tags/环境搭建/"},{"name":"opencv3","slug":"opencv3","permalink":"https://haxisnake.github.io/tags/opencv3/"}]},{"title":"树莓派3B+编译安装opencv3","slug":"树莓派3B-编译安装opencv3","date":"2018-07-17T11:17:56.000Z","updated":"2018-07-17T11:55:21.359Z","comments":true,"path":"2018/07/17/树莓派3B-编译安装opencv3/","link":"","permalink":"https://haxisnake.github.io/2018/07/17/树莓派3B-编译安装opencv3/","excerpt":"","text":"一、更新源12mv sources.list /etc/apt/sources.list mv raspi.list /etc/apt/sources.list.d/raspi.list 更新源的配置，注意文件存放的位置文件sources.list和raspi.list具体内容如下 sources.list文件:1234deb http://mirrors.ustc.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpideb http://mirrors.aliyun.com/raspbian/raspbian/ stretch main contrib non-free rpideb http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpideb http://mirrors.neusoft.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi raspi.list文件:1234deb http://mirrors.ustc.edu.cn/raspbian/raspbian/ stretch main uideb http://mirrors.aliyun.com/raspbian/raspbian/ stretch main uideb http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ stretch main uideb http://mirrors.neusoft.edu.cn/raspbian/raspbian/ stretch main ui 在终端执行更新命令:12sudo apt-get updatesudo apt-get upgrade 二、安装依赖包123456789sudo apt-get install build-essential cmake git pkg-config sudo apt-get install libjpeg8-dev sudo apt-get install libtiff5-dev sudo apt-get install libjasper-dev sudo apt-get install libpng12-devsudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-devsudo apt-get install libgtk2.0-devsudo apt-get install libatlas-base-dev gfortran 注意：降级安装有些安装包依赖的版本低需要降级安装，如下，对depends后面的进行降级安装1sudo aptitude install xxxx 三、下载源码1git clone https://github.com/opencv/opencv.git 四、编译1234cmake dir/of/opencv/sourcesudo make -j4 sudo make installsudo ldconfig","categories":[{"name":"调试记录","slug":"调试记录","permalink":"https://haxisnake.github.io/categories/调试记录/"}],"tags":[{"name":"环境搭建","slug":"环境搭建","permalink":"https://haxisnake.github.io/tags/环境搭建/"},{"name":"opencv3","slug":"opencv3","permalink":"https://haxisnake.github.io/tags/opencv3/"},{"name":"raspberry3B+","slug":"raspberry3B","permalink":"https://haxisnake.github.io/tags/raspberry3B/"}]}]}